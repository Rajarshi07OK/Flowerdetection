--------------------------------------------------------------------------------
OVERALL METRICS
--------------------------------------------------------------------------------

mAP@0.5:       0.6790 (67.90%)
mAP@0.5:0.95:  0.1958 (19.58%)
Precision:     0.8882 (88.82%)
Recall:        0.7333 (73.33%)
F1 Score:      0.62 (62.00%)

--------------------------------------------------------------------------------
METRIC DEFINITIONS
--------------------------------------------------------------------------------

mAP (Mean Average Precision):
  - mAP@0.5: Average precision at IoU threshold of 0.5
  - mAP@0.5:0.95: Average precision across IoU thresholds from 0.5 to 0.95
  - Higher is better (closer to 1.0 or 100%)

Precision:
  - Ratio of correct positive predictions to total positive predictions
  - Answers: 'Of all detections, how many were correct?'
  - Higher precision = fewer false positives

Recall:
  - Ratio of correct positive predictions to all actual positives
  - Answers: 'Of all ground truth objects, how many were detected?'
  - Higher recall = fewer missed detections

F1 Score:
  - Harmonic mean of precision and recall
  - Balances both metrics (useful when both are important)

--------------------------------------------------------------------------------
CLASSES
--------------------------------------------------------------------------------
  Class 0: flowers

--------------------------------------------------------------------------------
MODEL PERFORMANCE SUMMARY
--------------------------------------------------------------------------------

Overall Performance: Good

The model achieved a mAP@0.5 of 67.90%, which is considered
good for object detection tasks.

The model has higher precision than recall, meaning it's conservative
in making predictions (fewer false positives, but may miss some objects).
